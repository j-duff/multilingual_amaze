{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j-duff/multilingual_amaze/blob/main/Multilingual_A_maze_Alternative_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multilingual A-Maze foil generation using BERT-like language models and wordfreq"
      ],
      "metadata": {
        "id": "Z73vI8yCq3Xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preliminaries\n",
        "Please run the following cells to install and import the necessary libraries."
      ],
      "metadata": {
        "id": "mOTp2Xr4spnF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV5teS5-uEoC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install minicons\n",
        "!pip install wordfreq\n",
        "!pip install unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVtYtp7xuWv7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "from minicons import scorer\n",
        "import torch\n",
        "\n",
        "from wordfreq import get_frequency_dict, zipf_frequency\n",
        "import unicodedata\n",
        "\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "\n",
        "from google.colab import files\n",
        "import csv\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Selecting a Minicons language model\n",
        "Please run the following cell and input the language model you would like to use for the experiment. It should be a masked language model, like BERT.\n"
      ],
      "metadata": {
        "id": "Mzcm2K-js8Lx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mRA16lk3ESC"
      },
      "outputs": [],
      "source": [
        "langmodel = input(\"What minicons language model would you like to use?\\nYou can select any from this list: https://huggingface.co/models\\nThe name of the model can be copied using the clipboard icon next to the name on the webpage.\\n\")\n",
        "print(langmodel, \"selected as model.\")\n",
        "model = scorer.MaskedLMScorer(langmodel, 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Selecting frequency information\n",
        "\n",
        "Please run the following cell to specify how you would like collect frequency information for the experiment, and to define a frequency band for the purpose of computing similarly-frequent words."
      ],
      "metadata": {
        "id": "svnd1MjoXuh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strict_scripts = {\n",
        "    # map 2-letter ISO codes to the Unicode script tags included in character names\n",
        "    # add more languages here as desired!\n",
        "    \"ar\": \"ARABIC\",\n",
        "    \"he\": \"HEBREW\",\n",
        "    \"en\": \"LATIN\"\n",
        "}\n",
        "\n",
        "def script_check(word, script):\n",
        "  # ensure first non-punctuation character in token has appropriate script\n",
        "  return script in unicodedata.name(word.strip()[0])\n",
        "\n",
        "freq_type = input(\"What type of frequency information would you like to use?\\nYou can select from the following options:\\n- wf: uses the wordfreq package, which provides multi-corpus frequency estimates for over 40 languages.\\n- csv: requires upload of a csv specifying your own vocabulary and frequency counts.\\n\")\n",
        "\n",
        "if freq_type == \"wf\":\n",
        "  # Use the wordfreq package\n",
        "  lang_code = input(\"What wordfreq language would you like to use?\\nYou can select any from the list here: https://pypi.org/project/wordfreq/. Use the two letter ISO code to reference your language.\\n\")\n",
        "  freq_dict_raw = get_frequency_dict(lang=lang_code, wordlist = \"best\")\n",
        "  script = strict_scripts.get(lang_code, None)\n",
        "  if script:\n",
        "    freq_dict = dict((x, zipf_frequency(x, lang=lang_code)) for x,y in freq_dict_raw.items() if script_check(x, script)) # convert to Zipf scale (base-10 logarithm of frequency per billion words)\n",
        "  else:\n",
        "    freq_dict = dict((x, zipf_frequency(x, lang=lang_code)) for x,y in freq_dict_raw.items()) # convert to Zipf scale (base-10 logarithm of frequency per billion words)\n",
        "  freq_window = float(input(\"wordfreq reports frequencies on the Zipf scale, the base-10 logarithm of frequency per billion words.\\nWhat is the window of frequency on this scale that you would like to use to consider words 'similar' frequency?\\nE.g., with a window of 1 Zipf, the word 'glove', with a Zipf of about 4 (10 per million), could match the words:\\n-'boast', Zipf of 3 (1 per million)\\n-'floor', Zipf of 5 (100 per million)\\n\"))\n",
        "elif freq_type == \"csv\":\n",
        "  # Upload a csv\n",
        "  print(\"Please upload the csv that contains the word-to-frequency mapping.\\nIt should have two columns, labeled 'word' and 'frequency'.\")\n",
        "  uploaded = files.upload()\n",
        "  freq_file = next(iter(uploaded))\n",
        "  freq_window = int(input(\"Given the frequency values you used in your input data, what is the window of frequency on this scale that you would like to use to consider words 'similar' frequency?\\nE.g., if your data provides frequencies per million, at a window of 10, the word 'glove', with a frequency of about 10 per million, could match:\\n-'boast', 1 per million\\n-'fever', 20 per million\\n\"))\n",
        "else:\n",
        "  raise ValueError(\"Invalid frequency type.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "97lE90VRcSoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Providing your stimuli\n",
        "Please run the following cells to upload your stimuli. They should be in a single-column CV, with the column labeled \"sentences\". More functionality to come."
      ],
      "metadata": {
        "id": "r4wMAoh0thYs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVdmj6QGdi-P"
      },
      "outputs": [],
      "source": [
        "print(\"Please upload your file that contains the stimuli sentences to be used for alternative generation.\")\n",
        "uploaded = files.upload()\n",
        "stim_file = next(iter(uploaded))\n",
        "\n",
        "def process_stimuli_file(filename):\n",
        "  res = []\n",
        "  with open(filename, mode='r', encoding='utf-8-sig') as csv_file:\n",
        "      csv_reader = csv.DictReader(csv_file)\n",
        "      for row in csv_reader:\n",
        "          sent = row['sentences']\n",
        "          res.append(sent)\n",
        "  return res\n",
        "\n",
        "sentences = process_stimuli_file(stim_file)\n",
        "print(\"Stimuli saved. \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Main Functions\n",
        "- find_similar_frequency\n",
        "- calculate_surprisal\n",
        "- find_alternatives"
      ],
      "metadata": {
        "id": "CvXpnTGSyeQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rauIQVljSgC"
      },
      "outputs": [],
      "source": [
        "# these characters will be stripped from the beginning and ending of words\n",
        "# for the purposes of calculating frequency and surprisal\n",
        "# but they will be maintained and added back to all of the potential alternatives\n",
        "punctuation = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
        "           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_',\n",
        "           '`', '{', '|', '}', '~', '»', '«', '“', '”']\n",
        "punct_pattern = \"[\" + re.escape(\"\".join(punctuation)) + \"]\"\n",
        "\n",
        "# instead of random selection, provide a window for frequency selection\n",
        "# iterates over freq_dict until it has assembled {goal} words within {window}\n",
        "# or it has hit its maximum search count of {timeout} words\n",
        "def find_similar_frequency(raw_word, window, goal, timeout, verbose_mode):\n",
        "  res = set()\n",
        "\n",
        "  leading_punct, cased_word, trailing_punct = re.search(\"^(\"+punct_pattern+\"*)(.*?)(\"+punct_pattern+\"*)$\", raw_word).group(1, 2, 3)\n",
        "  word = cased_word.lower()\n",
        "\n",
        "  print('\\nword: ', cased_word)\n",
        "\n",
        "  words = list(freq_dict.items())\n",
        "  random.shuffle(words)\n",
        "\n",
        "  if word in freq_dict.keys():\n",
        "    word_frq = freq_dict[word]\n",
        "    if verbose_mode:\n",
        "      print('\\tFrequency found in list:', word_frq)\n",
        "    n = 0\n",
        "    attempt = 0\n",
        "    min_length = len(word)\n",
        "    max_length = len(word)\n",
        "    test_window = window\n",
        "\n",
        "    for w, f in words:\n",
        "      if w != word and len(w) >= min_length and len(w) <= max_length:\n",
        "        if word_frq < (f + test_window) and word_frq > (f - test_window):\n",
        "          if verbose_mode:\n",
        "            print('\\t\\tfound match:',w,f)\n",
        "          res.add((w, f))\n",
        "      n += 1\n",
        "\n",
        "      if len(res) == goal:\n",
        "        break\n",
        "\n",
        "      # relaxing length as needed\n",
        "      if n == timeout and attempt % 2 == 0:\n",
        "        if verbose_mode:\n",
        "          print('\\tNot enough words with current criteria.\\n\\tRelaxing length constraints by ±1.')\n",
        "        n = 0\n",
        "        attempt += 1\n",
        "        min_length -= 1\n",
        "        max_length += 1\n",
        "\n",
        "      # relaxing frequency window as needed\n",
        "      if n == timeout and attempt % 2 == 1:\n",
        "        if verbose_mode:\n",
        "          print('\\tNot enough words with current criteria.\\n\\tExpanding frequency window by 2x.')\n",
        "        n = 0\n",
        "        attempt += 1\n",
        "        test_window *= 2\n",
        "\n",
        "  else:\n",
        "    # error handling - word doesn't exist in given frequency list\n",
        "    # complete random selection\n",
        "    if verbose_mode:\n",
        "      print('\\tFrequency not found in list. Drawing a random sample based on length alone.')\n",
        "    n = 0\n",
        "    for w, f in words:\n",
        "      if w != word and len(w)==len(word):\n",
        "        if verbose_mode:\n",
        "          print('\\t\\tfound match:',w,f)\n",
        "        res.add((w, f))\n",
        "      n += 1\n",
        "      if n == timeout or len(res) == goal:\n",
        "        break\n",
        "\n",
        "  if cased_word.istitle():\n",
        "    return [(leading_punct+w.capitalize()+trailing_punct, f) for w,f in res]\n",
        "  elif cased_word.isupper():\n",
        "    return [(leading_punct+w.upper()+trailing_punct, f) for w,f in res]\n",
        "  else:\n",
        "    return [(leading_punct+w+trailing_punct, f) for w,f in res]\n",
        "\n",
        "def calculate_surprisal(target, candidates, prefix, suffix, n_highest, verbose_mode):\n",
        "    inputs = [target] + candidates\n",
        "\n",
        "    # calculate surprisal of each word in inputs\n",
        "    print('\\tCalculating surprisals...')\n",
        "    logprobs = model.conditional_score(prefix=[prefix]*len(inputs),\n",
        "                                         stimuli=[w for w,_ in inputs],\n",
        "                                         suffix=[suffix]*len(inputs),\n",
        "                                         reduction=lambda x: x.sum(0),\n",
        "                                         base_two=True)\n",
        "\n",
        "    target_data = [target[0], target[1], -1*logprobs[0].tolist()]\n",
        "\n",
        "    # now we have a list of surprisals, find the highest n\n",
        "    foil_surprisals = [-1*logprob.tolist() for logprob in logprobs[1:]]\n",
        "    max_indexes = sorted(range(len(foil_surprisals)), key=lambda i: foil_surprisals[i])[-n_highest:]\n",
        "    max_indexes.reverse()\n",
        "    foil_data = [(candidates[index][0], candidates[index][1], foil_surprisals[index]) for index in max_indexes]\n",
        "    if verbose_mode:\n",
        "      print('\\tBest candidates chosen:')\n",
        "      for output in foil_data:\n",
        "        print('\\t\\t', output[0], '\\t surprisal: ', output[2])\n",
        "    return target_data, foil_data # a [word, freq, surp] list and a list of [foil, freq, surp] lists\n",
        "\n",
        "def find_alternatives(target, candidates, prefix, suffix, n_highest=5, verbose_mode=False):\n",
        "  target_freq = freq_dict.get(target, 0)\n",
        "  target_data, foil_data = calculate_surprisal((target, target_freq), candidates, prefix, suffix, n_highest, verbose_mode)\n",
        "\n",
        "  result = [target_data + [n+1, alternative[0], alternative[1], alternative[2]] for n, alternative in enumerate(foil_data)]\n",
        "  return result # list of [target, target_freq, target_surp, foil_rank, foil, foil_freq, foil_surp] lists"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Alternative Generation\n",
        "\n",
        "This block runs the alternate generation and creates an output file under the name of your choosing.\n",
        "\n",
        "Recommendations: 100 candidate foils, save 5.\n",
        "\n",
        "But note: Evaluating 100 candidate foils takes about 4-5 minutes per sentence. Plan accordingly."
      ],
      "metadata": {
        "id": "WIIK-JPi5MSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_goal = int(input(\"How many possible frequency-matched foils do you want to sample? (Recommended: 100) \"))\n",
        "user_n = int(input(\"How many alternative foils do you want to save for each word? (Recommended: 5) \"))\n",
        "\n",
        "outfile_name = input(\"What is the name of your output file? \")\n",
        "f = open(outfile_name, mode='a', encoding='utf-8-sig')\n",
        "writer = csv.writer(f, quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "print('\\nBeginning generation...\\n')\n",
        "\n",
        "writer.writerow(['sentence_id', 'word_id', 'target', 'target_freq', 'target_surp', 'foil_rank', 'foil', 'foil_freq', 'foil_surp'])\n",
        "for sentence_id in range(len(sentences)):\n",
        "  sentence = sentences[sentence_id]\n",
        "  split = sentence.split()\n",
        "  for word_id in range(1, len(split)):\n",
        "    target = split[word_id]\n",
        "    prefix = \" \".join(split[:word_id])\n",
        "    suffix = \" \".join(split[word_id+1:])\n",
        "    candidates = find_similar_frequency(target, freq_window, goal=user_goal, timeout=10000, verbose_mode=True)\n",
        "    result = find_alternatives(target, candidates, prefix, suffix, n_highest=user_n, verbose_mode=True)\n",
        "    for output in result:\n",
        "      writer.writerow([sentence_id, word_id] + output)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "rcqGMNAa5G6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmqJEPT3DhW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}